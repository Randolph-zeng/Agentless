{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "def request_openai(client, prompt, model, max_retry=1, temperature=1., top_p=0.9, max_token=2048, batch_size=1):\n",
    "    message_dict = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "    messages = []\n",
    "    messages.append(message_dict)\n",
    "    retry = 0\n",
    "    while True:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                            model=model,\n",
    "                            messages=messages,\n",
    "                            max_tokens=max_token,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            n=batch_size\n",
    "                            \n",
    "            )\n",
    "            # 从响应中提取对话回复\n",
    "            output = completion.choices[0].message.content\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if retry < max_retry:\n",
    "                print(f\"Exception occurred, wait 3s: {e}\", flush=True)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                output = ''\n",
    "                break\n",
    "            retry += 1\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "model = \"deepseek-coder\"\n",
    "# prompt = 'write me a quicksort algorithm in python.'\n",
    "eval_client = OpenAI(base_url=\"https://api.deepseek.com\", api_key='sk-8c1dd14f03e24d6197d660908dc2de4c')\n",
    "# res = request_openai(eval_client, prompt, model, batch_size=3)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWE Bench Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# swe_bench_data = load_dataset(\"princeton-nlp/SWE-bench\", split=\"train\")  # Too slow \n",
    "swe_bench_data = load_dataset(\"/home/llm/.cache/huggingface/datasets/princeton-nlp___swe-bench/default/0.0.0/f5351ee8c6663736817027db3ad03fe662cb5bb8\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Ground Truth Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_git_patch(patch):\n",
    "    results = []\n",
    "    \n",
    "    # Regex to capture the file names and hunk information\n",
    "    file_pattern = re.compile(r'diff --git a/(.+) b/(.+)')\n",
    "    hunk_pattern = re.compile(r'@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@(?: (.*))?')\n",
    "\n",
    "    # Split the patch by 'diff --git' to handle each file separately\n",
    "    file_match_locs = [m for m in re.finditer(file_pattern, patch)]\n",
    "    for f_m_idx, f_match in enumerate(file_match_locs):\n",
    "        file_start_idx = f_match.start()\n",
    "        file_end_idx = file_match_locs[f_m_idx+1].start() if (f_m_idx+1) < len(file_match_locs) else None\n",
    "        modified_file_content = patch[file_start_idx:file_end_idx]\n",
    "        # get match file path info\n",
    "        orig_file_path, new_file_path = f_match.groups() \n",
    "        # split each modified file by the git patch header\n",
    "        hunk_match_locs =  [m for m in re.finditer(hunk_pattern, modified_file_content)]\n",
    "        for h_m_idx, h_match in enumerate(hunk_match_locs):\n",
    "            hunk_start_idx = h_match.start()\n",
    "            hunk_end_idx = hunk_match_locs[h_m_idx+1].start() if (h_m_idx+1) < len(hunk_match_locs) else None\n",
    "            modified_hunk_content = modified_file_content[hunk_start_idx:hunk_end_idx]\n",
    "            # Get the hunk header info \n",
    "            orig_start_line, orig_line_count, new_start_line, new_line_count, hunk_context = re.findall(hunk_pattern, modified_hunk_content)[0]\n",
    "            # get original and new content\n",
    "            hunk_lines = modified_file_content[h_match.end(): hunk_end_idx].split('\\n')\n",
    "            orig_hunk_lines, new_hunk_lines = [], []\n",
    "            for line in hunk_lines:\n",
    "                if line == '\\\\ No newline at end of file': continue # ignore the format string added by git\n",
    "                if line and not line.startswith('+'):\n",
    "                    if line.startswith('-'):\n",
    "                        orig_hunk_lines.append(line[1:])\n",
    "                    else:\n",
    "                        orig_hunk_lines.append(line)\n",
    "                if line and not line.startswith('-'):\n",
    "                    if line.startswith('+'):\n",
    "                        new_hunk_lines.append(line[1:])\n",
    "                    else:\n",
    "                        new_hunk_lines.append(line)\n",
    "            results.append({\n",
    "                \"original_file_path\": orig_file_path,\n",
    "                \"new_file_path\": new_file_path,\n",
    "                \"original_start_line\": orig_start_line,\n",
    "                \"new_start_line\": new_start_line,\n",
    "                \"hunk_context\": hunk_context,\n",
    "                \"original_hunk_lines\": orig_hunk_lines,\n",
    "                \"new_hunk_lines\": new_hunk_lines,\n",
    "                \"original_line_count\":orig_line_count,\n",
    "                \"new_line_count\": new_line_count,\n",
    "                \"hunk_content\": modified_hunk_content\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for bench_data in tqdm(swe_bench_data):\n",
    "    patch = bench_data['patch']\n",
    "    parsed_patch_info = parse_git_patch(patch)    \n",
    "    mismatch = False\n",
    "    # for info in parsed_patch_info:\n",
    "    #     if not (abs(int(info[\"original_line_count\"])-len(info[\"original_hunk_lines\"])) <= 1\n",
    "    #             and abs(int(info[\"new_line_count\"])-len(info[\"new_hunk_lines\"])) <=1):\n",
    "    #         mismatch = True # This happens when the patch itself contains git header info\n",
    "    if not len(parsed_patch_info): count +=1\n",
    "count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locate File Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_repo_structure.get_repo_structure import (get_project_structure_from_scratch)\n",
    "from agentless.util.preprocess_data import (show_project_structure)\n",
    "\n",
    "bench_data = swe_bench_data[0]\n",
    "d = get_project_structure_from_scratch(\n",
    "        bench_data[\"repo\"], bench_data[\"base_commit\"],bench_data[\"instance_id\"] , \"/dataset/zszeng/AgentlessOutputs/playground\"\n",
    ")\n",
    "patch_info = parse_git_patch(bench_data['patch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtain_relevant_files_prompt = \"\"\"\n",
    "Please look through the following GitHub problem description and Repository structure and provide a list of files that one would need to edit to fix the problem.\n",
    "\n",
    "### GitHub Problem Description ###\n",
    "{problem_statement}\n",
    "\n",
    "###\n",
    "\n",
    "### Repository Structure ###\n",
    "{structure}\n",
    "\n",
    "###\n",
    "\n",
    "Based on the problem description and repo structure, please give a brief analysis on which set of files are necessary to edit, then retrieve the relevant directory structures and return at most 10 files in full path.\n",
    "Following is the desired format:\n",
    "### \n",
    "Analysis: [Insert a brief analysis on which set of files are necessary to edit based on the problem description and repo structure]\n",
    "###\n",
    "Relevant Directories: [Retrieve the **COMPLETE** relevant directories strcuture from the Repository Structure here based on your analysis. Make sure the retrieved directory IS NOT a sub directory but contains the root folder!]\n",
    "###\n",
    "Relevant File Paths: [Put the **FULL PATHS** of the files that are relevant to the problem here. Each path should be in a single line. Return the necessary files only but limit the maximum number of files to 10.]\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "obtain_relevant_files_prompt_with_hint = \"\"\"\n",
    "Please look through the following GitHub problem description and Repository structure and provide a list of files that one would need to edit to fix the problem.\n",
    "\n",
    "### GitHub Problem Description ###\n",
    "{problem_statement}\n",
    "\n",
    "###\n",
    "\n",
    "### Repository Structure ###\n",
    "{structure}\n",
    "\n",
    "###\n",
    "\n",
    "Based on the problem description and repo structure, please give a brief analysis on which set of files are necessary to edit, then retrieve the relevant directory structures and return at most 10 files in full path.\n",
    "Following is the desired format:\n",
    "### \n",
    "Analysis: [Insert a brief analysis on which set of files are necessary to edit based on the problem description and repo structure]\n",
    "###\n",
    "Relevant Directories: [Retrieve the **COMPLETE** relevant directories strcuture from the Repository Structure here based on your analysis. Make sure the retrieved directory IS NOT a sub directory but contains the root folder!]\n",
    "###\n",
    "Relevant File Paths: [Put the **FULL PATHS** of the files that are relevant to the problem here. Each path should be in a single line. Return the necessary files only but limit the maximum number of files to 10.]\n",
    "###\n",
    "\n",
    "Hint: The followings are the ground truth files that need to be modified, please construct your formatted response based on this info:\n",
    "{ground_truth_modified_files}\n",
    "\"\"\"\n",
    "ground_truth_modified_files = set([info[\"original_file_path\"] for info in patch_info])\n",
    "\n",
    "message = obtain_relevant_files_prompt.format(\n",
    "            problem_statement=bench_data[\"problem_statement\"],\n",
    "            structure=show_project_structure(d[\"structure\"]).strip(),\n",
    ").strip()\n",
    "\n",
    "# message = obtain_relevant_files_prompt_with_hint.format(\n",
    "#             problem_statement=bench_data[\"problem_statement\"],\n",
    "#             structure=show_project_structure(d[\"structure\"]).strip(),\n",
    "#             ground_truth_modified_files=\"\\n\".join(ground_truth_modified_files)\n",
    "# ).strip()\n",
    "\n",
    "\n",
    "res = request_openai(eval_client, message, model, temperature=0.5)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_project_structure(d[\"structure\"]).strip())\n",
    "# ground_truth_modified_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locate to functions/class/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentless.util.preprocess_data import (\n",
    "    correct_file_paths,\n",
    "    get_full_file_paths_and_classes_and_functions,\n",
    "    get_repo_files\n",
    ")\n",
    "from agentless.util.compress_file import get_skeleton\n",
    "file_content_in_block_template = \"\"\"\n",
    "### File: {file_name} ###\n",
    "```python\n",
    "{file_content}\n",
    "```\n",
    "\"\"\"\n",
    "obtain_relevant_functions_and_vars_from_compressed_files_prompt_more = \"\"\"\n",
    "Please look through the following GitHub Problem Description and the Skeleton of Relevant Files.\n",
    "Identify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\n",
    "For each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n",
    "\n",
    "### GitHub Problem Description ###\n",
    "{problem_statement}\n",
    "\n",
    "### Skeleton of Relevant Files ###\n",
    "{file_contents}\n",
    "\n",
    "###\n",
    "\n",
    "Please provide the complete set of locations as either a class name, a function name, or a variable name.\n",
    "Note that if you include a class, you do not need to list its specific methods.\n",
    "You can include either the entire class or don't include the class name and instead include specific methods in the class.\n",
    "### Examples:\n",
    "```\n",
    "full_path1/file1.py\n",
    "function: my_function_1\n",
    "class: MyClass1\n",
    "function: MyClass2.my_method\n",
    "\n",
    "full_path2/file2.py\n",
    "variable: my_var\n",
    "function: MyClass3.my_method\n",
    "\n",
    "full_path3/file3.py\n",
    "function: my_function_2\n",
    "function: my_function_3\n",
    "function: MyClass4.my_method_1\n",
    "class: MyClass5\n",
    "```\n",
    "\n",
    "Return just the locations.\n",
    "\"\"\"\n",
    "model_found_files = res.split(\"Relevant File Paths:\")[-1].strip().split(\"\\n\")\n",
    "files, classes, functions = get_full_file_paths_and_classes_and_functions(d[\"structure\"])\n",
    "\n",
    "# sort based on order of appearance in model_found_files\n",
    "found_files = correct_file_paths(model_found_files, files, True)\n",
    "\n",
    "file_contents = get_repo_files(d[\"structure\"], found_files)\n",
    "compressed_file_contents = {\n",
    "    fn: get_skeleton(code) for fn, code in file_contents.items()\n",
    "}\n",
    "contents = [\n",
    "    file_content_in_block_template.format(file_name=fn, file_content=code)\n",
    "    for fn, code in compressed_file_contents.items()\n",
    "]\n",
    "file_contents = \"\".join(contents)\n",
    "template = (\n",
    "    obtain_relevant_functions_and_vars_from_compressed_files_prompt_more\n",
    ")\n",
    "message = template.format(\n",
    "    problem_statement=bench_data[\"problem_statement\"], file_contents=file_contents\n",
    ")\n",
    "res = request_openai(eval_client, message, model, temperature=0.)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([\"3\",\"2\", \"1\"])  == set([\"2\", \"1\", \"3\" ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
